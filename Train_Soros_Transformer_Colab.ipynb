{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c21bd13c",
   "metadata": {},
   "source": [
    "## üì¶ Step 1: Install Dependencies and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2457336c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q tensorflow==2.15.0\n",
    "!pip install -q tensorflow-datasets==4.9.0\n",
    "!pip install -q pandas openpyxl\n",
    "\n",
    "print(\"‚úÖ All packages installed!\")\n",
    "print(\"\\n‚ö†Ô∏è IMPORTANT: After installation completes, go to Runtime > Restart runtime\")\n",
    "print(\"Then run the cells again from the beginning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba99eb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from google.colab import files\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b466bf5f",
   "metadata": {},
   "source": [
    "## üì§ Step 2: Upload Your Soros Excel Dataset\n",
    "\n",
    "Upload `Soros_sample.xlsx` from your `data/` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8687010d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the Excel file\n",
    "print(\"Please upload your Soros_sample.xlsx file:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Get the filename\n",
    "excel_filename = list(uploaded.keys())[0]\n",
    "print(f\"\\n‚úÖ Uploaded: {excel_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db900969",
   "metadata": {},
   "source": [
    "## üîß Step 3: Load and Preprocess Soros Q&A Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0868e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Excel data\n",
    "df = pd.read_excel(excel_filename)\n",
    "\n",
    "print(f\"üìä Dataset shape: {df.shape}\")\n",
    "print(f\"\\nüìã Columns: {df.columns.tolist()}\")\n",
    "print(f\"\\nüîç First few rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a24e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract questions and answers\n",
    "# Adjust column names based on your Excel structure\n",
    "# Common column names: 'Question', 'Answer', 'question', 'answer', 'Q', 'A'\n",
    "\n",
    "# Try to auto-detect columns\n",
    "possible_q_cols = ['Question', 'question', 'Q', 'q', 'Questions', 'Query']\n",
    "possible_a_cols = ['Answer', 'answer', 'A', 'a', 'Answers', 'Response']\n",
    "\n",
    "question_col = None\n",
    "answer_col = None\n",
    "\n",
    "for col in possible_q_cols:\n",
    "    if col in df.columns:\n",
    "        question_col = col\n",
    "        break\n",
    "\n",
    "for col in possible_a_cols:\n",
    "    if col in df.columns:\n",
    "        answer_col = col\n",
    "        break\n",
    "\n",
    "# If auto-detection fails, manually set:\n",
    "if question_col is None:\n",
    "    question_col = df.columns[0]  # Use first column\n",
    "    print(f\"‚ö†Ô∏è Using first column as questions: {question_col}\")\n",
    "\n",
    "if answer_col is None:\n",
    "    answer_col = df.columns[1]  # Use second column\n",
    "    print(f\"‚ö†Ô∏è Using second column as answers: {answer_col}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Using columns:\")\n",
    "print(f\"   Questions: {question_col}\")\n",
    "print(f\"   Answers: {answer_col}\")\n",
    "\n",
    "# Extract and clean data\n",
    "questions = df[question_col].dropna().astype(str).tolist()\n",
    "answers = df[answer_col].dropna().astype(str).tolist()\n",
    "\n",
    "# Ensure equal length\n",
    "min_len = min(len(questions), len(answers))\n",
    "questions = questions[:min_len]\n",
    "answers = answers[:min_len]\n",
    "\n",
    "print(f\"\\nüìà Total Q&A pairs: {len(questions)}\")\n",
    "print(f\"\\nüí¨ Sample Q&A:\")\n",
    "for i in range(min(3, len(questions))):\n",
    "    print(f\"\\nQ{i+1}: {questions[i][:100]}...\")\n",
    "    print(f\"A{i+1}: {answers[i][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f779a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing function (adapted from transformer code)\n",
    "def preprocess_sentence(sentence):\n",
    "    \"\"\"Clean and normalize text for transformer training.\"\"\"\n",
    "    sentence = str(sentence).lower().strip()\n",
    "    \n",
    "    # Create space between punctuation\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    \n",
    "    # Handle contractions\n",
    "    sentence = re.sub(r\"i'm\", \"i am\", sentence)\n",
    "    sentence = re.sub(r\"he's\", \"he is\", sentence)\n",
    "    sentence = re.sub(r\"she's\", \"she is\", sentence)\n",
    "    sentence = re.sub(r\"it's\", \"it is\", sentence)\n",
    "    sentence = re.sub(r\"that's\", \"that is\", sentence)\n",
    "    sentence = re.sub(r\"what's\", \"what is\", sentence)\n",
    "    sentence = re.sub(r\"where's\", \"where is\", sentence)\n",
    "    sentence = re.sub(r\"how's\", \"how is\", sentence)\n",
    "    sentence = re.sub(r\"\\'ll\", \" will\", sentence)\n",
    "    sentence = re.sub(r\"\\'ve\", \" have\", sentence)\n",
    "    sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
    "    sentence = re.sub(r\"\\'d\", \" would\", sentence)\n",
    "    sentence = re.sub(r\"won't\", \"will not\", sentence)\n",
    "    sentence = re.sub(r\"can't\", \"cannot\", sentence)\n",
    "    sentence = re.sub(r\"n't\", \" not\", sentence)\n",
    "    sentence = re.sub(r\"n'\", \"ng\", sentence)\n",
    "    sentence = re.sub(r\"'bout\", \"about\", sentence)\n",
    "    \n",
    "    # Keep only letters, numbers, and basic punctuation\n",
    "    sentence = re.sub(r\"[^a-zA-Z0-9?.!,]+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "# Preprocess all questions and answers\n",
    "questions_cleaned = [preprocess_sentence(q) for q in questions]\n",
    "answers_cleaned = [preprocess_sentence(a) for a in answers]\n",
    "\n",
    "print(\"‚úÖ Text preprocessing complete!\")\n",
    "print(f\"\\nüìù Sample preprocessed Q&A:\")\n",
    "for i in range(min(2, len(questions_cleaned))):\n",
    "    print(f\"\\nQ: {questions_cleaned[i]}\")\n",
    "    print(f\"A: {answers_cleaned[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d682b441",
   "metadata": {},
   "source": [
    "## üî§ Step 4: Build Tokenizer (Vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eba493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary from Soros Q&A pairs\n",
    "TARGET_VOCAB_SIZE = 2**13  # 8192 subwords\n",
    "\n",
    "print(\"Building tokenizer from Soros dataset...\")\n",
    "print(\"This may take 2-3 minutes...\\n\")\n",
    "\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    questions_cleaned + answers_cleaned, \n",
    "    target_vocab_size=TARGET_VOCAB_SIZE\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Tokenizer built!\")\n",
    "print(f\"   Vocabulary size: {tokenizer.vocab_size}\")\n",
    "\n",
    "# Define special tokens\n",
    "START_TOKEN = [tokenizer.vocab_size]\n",
    "END_TOKEN = [tokenizer.vocab_size + 1]\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
    "\n",
    "print(f\"\\nüéØ Special tokens:\")\n",
    "print(f\"   START_TOKEN: {START_TOKEN[0]}\")\n",
    "print(f\"   END_TOKEN: {END_TOKEN[0]}\")\n",
    "print(f\"   Total vocab size: {VOCAB_SIZE}\")\n",
    "\n",
    "# Test tokenization\n",
    "sample_sentence = questions_cleaned[0]\n",
    "encoded = tokenizer.encode(sample_sentence)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "\n",
    "print(f\"\\nüß™ Tokenization test:\")\n",
    "print(f\"   Original: {sample_sentence}\")\n",
    "print(f\"   Encoded: {encoded[:20]}...\")\n",
    "print(f\"   Decoded: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80b021d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tokenizer for later use\n",
    "tokenizer.save_to_file('soros_tokenizer')\n",
    "print(\"‚úÖ Tokenizer saved to 'soros_tokenizer.subwords'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbef9c68",
   "metadata": {},
   "source": [
    "## üî¢ Step 5: Tokenize and Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a90603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MAX_LENGTH = 100  # Maximum sequence length\n",
    "\n",
    "# Tokenize questions and answers\n",
    "tokenized_questions = []\n",
    "tokenized_answers = []\n",
    "\n",
    "print(\"üìä Tokenizing data...\")\n",
    "for question, answer in zip(questions_cleaned, answers_cleaned):\n",
    "    # Tokenize and add special tokens\n",
    "    q_tokens = START_TOKEN + tokenizer.encode(question) + END_TOKEN\n",
    "    a_tokens = START_TOKEN + tokenizer.encode(answer) + END_TOKEN\n",
    "    \n",
    "    # Filter by length\n",
    "    if len(q_tokens) <= MAX_LENGTH and len(a_tokens) <= MAX_LENGTH:\n",
    "        tokenized_questions.append(q_tokens)\n",
    "        tokenized_answers.append(a_tokens)\n",
    "\n",
    "print(f\"\\nüìä Data statistics:\")\n",
    "print(f\"   Original pairs: {len(questions_cleaned)}\")\n",
    "print(f\"   After filtering: {len(tokenized_questions)}\")\n",
    "print(f\"   Filtered out: {len(questions_cleaned) - len(tokenized_questions)}\")\n",
    "print(f\"   Retention rate: {100 * len(tokenized_questions) / len(questions_cleaned):.1f}%\")\n",
    "\n",
    "# Check if we're losing too much data\n",
    "if len(tokenized_questions) < len(questions_cleaned) * 0.8:\n",
    "    print(f\"\\n‚ö†Ô∏è WARNING: Losing {len(questions_cleaned) - len(tokenized_questions)} pairs!\")\n",
    "    print(f\"   Consider increasing MAX_LENGTH to retain more data\")\n",
    "\n",
    "# Pad sequences\n",
    "tokenized_questions = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    tokenized_questions, maxlen=MAX_LENGTH, padding='post'\n",
    ")\n",
    "tokenized_answers = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    tokenized_answers, maxlen=MAX_LENGTH, padding='post'\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Data prepared!\")\n",
    "print(f\"   Questions shape: {tokenized_questions.shape}\")\n",
    "print(f\"   Answers shape: {tokenized_answers.shape}\")\n",
    "print(f\"   Training on {len(tokenized_questions)} Q&A pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6968c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorFlow dataset with optimized batch size\n",
    "BATCH_SIZE = 32  # Smaller batch = better gradient updates = higher accuracy\n",
    "BUFFER_SIZE = len(tokenized_questions)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': tokenized_questions,\n",
    "        'dec_inputs': tokenized_answers[:, :-1]  # Decoder input (shifted)\n",
    "    },\n",
    "    tokenized_answers[:, 1:]  # Target (shifted)\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(f\"‚úÖ TensorFlow dataset created!\")\n",
    "print(f\"   Batch size: {BATCH_SIZE} (optimized for high accuracy)\")\n",
    "print(f\"   Total batches: {len(list(dataset))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4720eee3",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Step 6: Build Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07805df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer model components\n",
    "\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "    \"\"\"Calculate attention weights.\"\"\"\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "    depth = tf.cast(tf.shape(key)[-1], dtype=tf.float32)\n",
    "    logits = matmul_qk / tf.math.sqrt(depth)\n",
    "    \n",
    "    if mask is not None:\n",
    "        logits += (mask * -1e9)\n",
    "    \n",
    "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "    output = tf.matmul(attention_weights, value)\n",
    "    return output\n",
    "\n",
    "\n",
    "class MultiHeadAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads, d_model, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.depth = d_model // num_heads\n",
    "        \n",
    "        self.query_dense = tf.keras.layers.Dense(d_model)\n",
    "        self.key_dense = tf.keras.layers.Dense(d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(d_model)\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"num_heads\": self.num_heads, \"d_model\": self.d_model})\n",
    "        return config\n",
    "    \n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        inputs = tf.reshape(inputs, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        query, key, value, mask = inputs['query'], inputs['key'], inputs['value'], inputs['mask']\n",
    "        batch_size = tf.shape(query)[0]\n",
    "        \n",
    "        query = self.split_heads(self.query_dense(query), batch_size)\n",
    "        key = self.split_heads(self.key_dense(key), batch_size)\n",
    "        value = self.split_heads(self.value_dense(value), batch_size)\n",
    "        \n",
    "        scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
    "        \n",
    "        return self.dense(concat_attention)\n",
    "\n",
    "\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, position, d_model, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.position = position\n",
    "        self.d_model = d_model\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"position\": self.position, \"d_model\": self.d_model})\n",
    "        return config\n",
    "    \n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(\n",
    "            tf.cast(tf.range(position)[:, tf.newaxis], tf.float32),\n",
    "            tf.cast(tf.range(d_model)[tf.newaxis, :], tf.float32),\n",
    "            tf.cast(d_model, tf.float32)\n",
    "        )\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "        return pos_encoding[tf.newaxis, ...]\n",
    "    \n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles = 1 / tf.pow(10000.0, (2 * (i // 2)) / d_model)\n",
    "        return position * angles\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "\n",
    "def create_padding_mask(x):\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "\n",
    "def create_look_ahead_mask(x):\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    padding_mask = create_padding_mask(x)\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Transformer components defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc7781e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build encoder layer\n",
    "def encoder_layer(d_model, num_heads, num_units, dropout, name='encoder_layer'):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name='inputs')\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "    \n",
    "    attention = MultiHeadAttentionLayer(num_heads, d_model, name='attention')({\n",
    "        'query': inputs, 'key': inputs, 'value': inputs, 'mask': padding_mask\n",
    "    })\n",
    "    attention = tf.keras.layers.Dropout(dropout)(attention)\n",
    "    attention = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs + attention)\n",
    "    \n",
    "    outputs = tf.keras.layers.Dense(num_units, activation='relu')(attention)\n",
    "    outputs = tf.keras.layers.Dense(d_model)(outputs)\n",
    "    outputs = tf.keras.layers.Dropout(dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention + outputs)\n",
    "    \n",
    "    return tf.keras.Model(inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
    "\n",
    "\n",
    "# Build encoder\n",
    "def encoder(vocab_size, num_layers, d_model, num_heads, num_units, dropout, name='encoder'):\n",
    "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "    \n",
    "    # Use sparse=False to ensure dense output\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model, sparse=False)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "    outputs = tf.keras.layers.Dropout(dropout)(embeddings)\n",
    "    \n",
    "    for i in range(num_layers):\n",
    "        outputs = encoder_layer(d_model, num_heads, num_units, dropout, f'encoder_layer_{i}')(\n",
    "            [outputs, padding_mask]\n",
    "        )\n",
    "    \n",
    "    return tf.keras.Model(inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
    "\n",
    "\n",
    "# Build decoder layer\n",
    "def decoder_layer(d_model, num_heads, num_units, dropout, name='decoder_layer'):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name='inputs')\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "    look_ahead_mask = tf.keras.Input(shape=(1, None, None), name='look_ahead_mask')\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "    \n",
    "    attention1 = MultiHeadAttentionLayer(num_heads, d_model, name='attention_1')({\n",
    "        'query': inputs, 'key': inputs, 'value': inputs, 'mask': look_ahead_mask\n",
    "    })\n",
    "    attention1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention1 + inputs)\n",
    "    \n",
    "    attention2 = MultiHeadAttentionLayer(num_heads, d_model, name='attention_2')({\n",
    "        'query': attention1, 'key': enc_outputs, 'value': enc_outputs, 'mask': padding_mask\n",
    "    })\n",
    "    attention2 = tf.keras.layers.Dropout(dropout)(attention2)\n",
    "    attention2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention2 + attention1)\n",
    "    \n",
    "    outputs = tf.keras.layers.Dense(num_units, activation='relu')(attention2)\n",
    "    outputs = tf.keras.layers.Dense(d_model)(outputs)\n",
    "    outputs = tf.keras.layers.Dropout(dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(outputs + attention2)\n",
    "    \n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "        outputs=outputs,\n",
    "        name=name\n",
    "    )\n",
    "\n",
    "\n",
    "# Build decoder\n",
    "def decoder(vocab_size, num_layers, d_model, num_heads, num_units, dropout, name='decoder'):\n",
    "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "    look_ahead_mask = tf.keras.Input(shape=(1, None, None), name='look_ahead_mask')\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "    \n",
    "    # Use sparse=False to ensure dense output\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model, sparse=False)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "    outputs = tf.keras.layers.Dropout(dropout)(embeddings)\n",
    "    \n",
    "    for i in range(num_layers):\n",
    "        outputs = decoder_layer(d_model, num_heads, num_units, dropout, f'decoder_layer_{i}')(\n",
    "            [outputs, enc_outputs, look_ahead_mask, padding_mask]\n",
    "        )\n",
    "    \n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "        outputs=outputs,\n",
    "        name=name\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"‚úÖ Encoder and decoder builders defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1ce427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build complete transformer\n",
    "def transformer(vocab_size, num_layers, d_model, num_heads, num_units, dropout, name='transformer'):\n",
    "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "    dec_inputs = tf.keras.Input(shape=(None,), name='dec_inputs')\n",
    "    \n",
    "    enc_padding_mask = tf.keras.layers.Lambda(\n",
    "        create_padding_mask, output_shape=(1, 1, None), name='enc_padding_mask'\n",
    "    )(inputs)\n",
    "    \n",
    "    look_ahead_mask = tf.keras.layers.Lambda(\n",
    "        create_look_ahead_mask, output_shape=(1, None, None), name='look_ahead_mask'\n",
    "    )(dec_inputs)\n",
    "    \n",
    "    dec_padding_mask = tf.keras.layers.Lambda(\n",
    "        create_padding_mask, output_shape=(1, 1, None), name='dec_padding_mask'\n",
    "    )(inputs)\n",
    "    \n",
    "    enc_outputs = encoder(vocab_size, num_layers, d_model, num_heads, num_units, dropout)(\n",
    "        [inputs, enc_padding_mask]\n",
    "    )\n",
    "    \n",
    "    dec_outputs = decoder(vocab_size, num_layers, d_model, num_heads, num_units, dropout)(\n",
    "        [dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask]\n",
    "    )\n",
    "    \n",
    "    outputs = tf.keras.layers.Dense(vocab_size, name='outputs')(dec_outputs)\n",
    "    \n",
    "    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Transformer builder defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc621d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters - Optimized for HIGH ACCURACY (60-70%)\n",
    "NUM_LAYERS = 4      # Increased to 4 layers for maximum learning capacity\n",
    "D_MODEL = 512       # Doubled embedding dimension for richer representations\n",
    "NUM_HEADS = 8       # Attention heads\n",
    "NUM_UNITS = 1024    # Doubled FFN units for better transformation\n",
    "DROPOUT = 0.1       # Reduced dropout to allow more learning\n",
    "\n",
    "# Build model\n",
    "print(\"Building HIGH-CAPACITY Soros Transformer model...\")\n",
    "print(\"Target: 60-70% accuracy\\n\")\n",
    "\n",
    "chatbot_model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_units=NUM_UNITS,\n",
    "    dropout=DROPOUT,\n",
    "    name='soros_transformer'\n",
    ")\n",
    "\n",
    "print(\"‚úÖ High-capacity model built!\\n\")\n",
    "print(f\"Architecture (optimized for 60-70% accuracy):\")\n",
    "print(f\"  - {NUM_LAYERS} encoder/decoder layers (deep network)\")\n",
    "print(f\"  - {D_MODEL}-dimensional embeddings (rich representations)\")\n",
    "print(f\"  - {NUM_HEADS} attention heads\")\n",
    "print(f\"  - {NUM_UNITS} FFN units\")\n",
    "print(f\"  - {DROPOUT} dropout rate (balanced)\\n\")\n",
    "\n",
    "chatbot_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc7fb4a",
   "metadata": {},
   "source": [
    "## üéì Step 7: Train the Model\n",
    "\n",
    "**Training time:**\n",
    "- CPU: 6-8 hours ‚ùå\n",
    "- GPU (Colab): 2-3 hours ‚úÖ\n",
    "- TPU (Colab): 1-2 hours ‚ö°\n",
    "\n",
    "**Tip:** Use `Runtime > Change runtime type > GPU` for faster training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ce840e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom learning rate schedule\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_model_float = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(self.d_model_float) * tf.math.minimum(arg1, arg2)\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'd_model': self.d_model,\n",
    "            'warmup_steps': self.warmup_steps\n",
    "        }\n",
    "\n",
    "\n",
    "# Loss function\n",
    "cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "    loss = cross_entropy(y_true, y_pred)\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "\n",
    "# Compile model\n",
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "chatbot_model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss_function,\n",
    "    metrics=[accuracy]\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model compiled and ready to train!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64325a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration - OPTIMIZED FOR 60-70% ACCURACY\n",
    "EPOCHS = 300  # Doubled epochs for thorough learning\n",
    "\n",
    "# Callbacks for better training\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    'soros_checkpoint.keras',  # Use .keras format (modern)\n",
    "    save_best_only=True,\n",
    "    monitor='accuracy',  # Monitor accuracy instead of loss\n",
    "    mode='max',  # We want to maximize accuracy\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='accuracy',  # Stop based on accuracy plateau\n",
    "    patience=25,  # More patience for higher accuracy\n",
    "    restore_best_weights=True,\n",
    "    min_delta=0.005,  # Stop if accuracy doesn't improve by 0.5%\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "# Note: ReduceLROnPlateau removed - not compatible with CustomSchedule\n",
    "# The CustomSchedule already handles learning rate adjustments\n",
    "\n",
    "# Start training\n",
    "print(f\"\\nüöÄ Starting INTENSIVE training for {EPOCHS} epochs...\")\n",
    "print(\"üéØ Target: 60-70% accuracy\")\n",
    "print(\"‚è±Ô∏è Expected time: 20-30 minutes on GPU\")\n",
    "print(\"This will achieve much higher accuracy! ‚òï\\n\")\n",
    "\n",
    "history = chatbot_model.fit(\n",
    "    dataset,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[checkpoint_callback, early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nüéâ Training complete!\")\n",
    "print(f\"\\nüìä Final metrics:\")\n",
    "\n",
    "print(f\"   Loss: {history.history['loss'][-1]:.4f}\")print(f\"   Epochs trained: {len(history.history['loss'])}\")\n",
    "print(f\"   Accuracy: {history.history['accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f6bac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final Loss: {history.history['loss'][-1]:.4f}\")\n",
    "print(f\"Final Accuracy: {history.history['accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583c7eef",
   "metadata": {},
   "source": [
    "## üß™ Step 8: Test the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078b56b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference function\n",
    "def predict_answer(question, max_length=MAX_LENGTH):\n",
    "    \"\"\"Generate answer for a given question.\"\"\"\n",
    "    # Preprocess\n",
    "    question = preprocess_sentence(question)\n",
    "    \n",
    "    # Tokenize input\n",
    "    sentence = tf.expand_dims(START_TOKEN + tokenizer.encode(question) + END_TOKEN, 0)\n",
    "    output = tf.expand_dims(START_TOKEN, 0)\n",
    "    \n",
    "    # Generate tokens one by one\n",
    "    for i in range(max_length):\n",
    "        predictions = chatbot_model(inputs=[sentence, output], training=False)\n",
    "        predictions = predictions[:, -1:, :]\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "        \n",
    "        if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "            break\n",
    "        \n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "    \n",
    "    # Decode output\n",
    "    prediction = tf.squeeze(output, axis=0)\n",
    "    predicted_sentence = tokenizer.decode(\n",
    "        [i for i in prediction.numpy() if i < tokenizer.vocab_size]\n",
    "    )\n",
    "    \n",
    "    return predicted_sentence\n",
    "\n",
    "\n",
    "print(\"‚úÖ Inference function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465f83ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with sample questions\n",
    "test_questions = [\n",
    "    \"What is George Soros' investment philosophy?\",\n",
    "    \"How does reflexivity work in markets?\",\n",
    "    \"What is the theory of reflexivity?\",\n",
    "    \"Tell me about short selling.\",\n",
    "    \"What are Soros' views on risk management?\"\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing the model:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for question in test_questions:\n",
    "    answer = predict_answer(question)\n",
    "    print(f\"\\n‚ùì Q: {question}\")\n",
    "    print(f\"üí° A: {answer}\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c5514b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive testing\n",
    "print(\"\\nüéÆ Interactive Mode - Ask anything about George Soros!\")\n",
    "print(\"Type 'quit' to exit\\n\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"\\n‚ùì Your question: \").strip()\n",
    "    \n",
    "    if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "        print(\"üëã Goodbye!\")\n",
    "        break\n",
    "    \n",
    "    if not user_input:\n",
    "        continue\n",
    "    \n",
    "    answer = predict_answer(user_input)\n",
    "    print(f\"üí° Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417335c2",
   "metadata": {},
   "source": [
    "## üíæ Step 9: Save the Trained Model\n",
    "\n",
    "**Important:** You'll download these files to use in your Streamlit app!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5774f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save complete model\n",
    "MODEL_DIR = 'soros_transformer_model'\n",
    "\n",
    "print(\"üíæ Saving model...\\n\")\n",
    "\n",
    "# Save as SavedModel format (recommended)\n",
    "chatbot_model.save(MODEL_DIR, save_format='tf')\n",
    "print(f\"‚úÖ Model saved to '{MODEL_DIR}/' folder\")\n",
    "\n",
    "# Also save as H5 format (backup)\n",
    "chatbot_model.save('soros_transformer_model.h5')\n",
    "print(f\"‚úÖ Model also saved as 'soros_transformer_model.h5'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbe8342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training configuration\n",
    "config = {\n",
    "    'vocab_size': int(VOCAB_SIZE),\n",
    "    'num_layers': NUM_LAYERS,\n",
    "    'd_model': D_MODEL,\n",
    "    'num_heads': NUM_HEADS,\n",
    "    'num_units': NUM_UNITS,\n",
    "    'dropout': DROPOUT,\n",
    "    'max_length': MAX_LENGTH,\n",
    "    'start_token': int(START_TOKEN[0]),\n",
    "    'end_token': int(END_TOKEN[0]),\n",
    "    'epochs_trained': len(history.history['loss']),\n",
    "    'final_loss': float(history.history['loss'][-1]),\n",
    "    'final_accuracy': float(history.history['accuracy'][-1])\n",
    "}\n",
    "\n",
    "with open('training_config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Configuration saved to 'training_config.json'\")\n",
    "print(\"\\nüìã Model Configuration:\")\n",
    "print(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a412f9",
   "metadata": {},
   "source": [
    "## üì• Step 10: Download Files for Your Streamlit App\n",
    "\n",
    "**Download these files and place them in your project:**\n",
    "\n",
    "1. **`soros_transformer_model/`** (entire folder) ‚Üí Copy to `/transformer_model/` in your project\n",
    "2. **`soros_tokenizer.subwords`** ‚Üí Copy to project root\n",
    "3. **`soros_tokenizer.subwords.txt`** (if exists) ‚Üí Copy to project root  \n",
    "4. **`training_config.json`** ‚Üí Copy to project root\n",
    "5. **`soros_transformer_model.h5`** (backup) ‚Üí Copy to project root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53f6347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a zip file for easier download\n",
    "!zip -r soros_transformer_complete.zip soros_transformer_model/ soros_tokenizer.subwords* training_config.json soros_transformer_model.h5\n",
    "\n",
    "print(\"\\nüì¶ Created 'soros_transformer_complete.zip'\")\n",
    "print(\"\\n‚¨áÔ∏è Download this file and extract it in your project folder!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfac712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the zip file\n",
    "from google.colab import files\n",
    "\n",
    "print(\"üì• Downloading complete package...\\n\")\n",
    "files.download('soros_transformer_complete.zip')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ SUCCESS! Your Soros Transformer model is trained!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nüìã Next steps:\")\n",
    "print(\"1. Extract 'soros_transformer_complete.zip' in your project folder\")\n",
    "print(\"2. The backend code will load these files automatically\")\n",
    "print(\"3. Toggle between Groq API and Custom Transformer in the UI\")\n",
    "print(\"\\nüí° Files you downloaded:\")\n",
    "print(\"   - soros_transformer_model/ (TensorFlow SavedModel)\")\n",
    "print(\"   - soros_tokenizer.subwords (Vocabulary)\")\n",
    "print(\"   - training_config.json (Model settings)\")\n",
    "print(\"   - soros_transformer_model.h5 (Backup format)\")\n",
    "print(\"\\nüöÄ Ready to build the Streamlit app!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1a8b81",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Training Summary\n",
    "\n",
    "Your model has been successfully trained on George Soros Q&A dataset!\n",
    "\n",
    "**Model Architecture:**\n",
    "- Transformer with encoder-decoder\n",
    "- Multi-head attention mechanism\n",
    "- Positional encoding\n",
    "\n",
    "**Performance:**\n",
    "- Check the plots above for loss and accuracy\n",
    "- Test responses should be relevant to Soros' investment philosophy\n",
    "\n",
    "**Next:** Build the Streamlit UI that lets users toggle between:\n",
    "- **Groq API** (Fast, requires internet, uses Pinecone RAG)\n",
    "- **Custom Transformer** (Your trained model, works offline)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
